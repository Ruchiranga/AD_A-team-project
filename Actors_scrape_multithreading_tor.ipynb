{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "import urllib.request\n",
    "import concurrent.futures\n",
    "from os import listdir\n",
    "from stem import Signal\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from os.path import isfile, join\n",
    "from stem.control import Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tor_session():\n",
    "    session = requests.session()\n",
    "    # Tor uses the 9050 port as the default socks port\n",
    "    session.proxies = {'http':  'socks5://127.0.0.1:9050',\n",
    "                       'https': 'socks5://127.0.0.1:9050'}\n",
    "    return session\n",
    "\n",
    "def renew_connection():\n",
    "    with Controller.from_port(port = 9051) as controller:\n",
    "        controller.authenticate(password=\"password\")\n",
    "        controller.signal(Signal.NEWNYM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_profession(str):\n",
    "    return str.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from pickle...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nconst</th>\n",
       "      <th>primaryName</th>\n",
       "      <th>birthYear</th>\n",
       "      <th>deathYear</th>\n",
       "      <th>primaryProfession</th>\n",
       "      <th>knownForTitles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nm0000001</td>\n",
       "      <td>Fred Astaire</td>\n",
       "      <td>1899</td>\n",
       "      <td>1987</td>\n",
       "      <td>actor</td>\n",
       "      <td>tt0050419,tt0053137,tt0043044,tt0072308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nm0000002</td>\n",
       "      <td>Lauren Bacall</td>\n",
       "      <td>1924</td>\n",
       "      <td>2014</td>\n",
       "      <td>actress</td>\n",
       "      <td>tt0117057,tt0037382,tt0071877,tt0038355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nm0000003</td>\n",
       "      <td>Brigitte Bardot</td>\n",
       "      <td>1934</td>\n",
       "      <td>\\N</td>\n",
       "      <td>actress</td>\n",
       "      <td>tt0049189,tt0059956,tt0054452,tt0057345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nm0000004</td>\n",
       "      <td>John Belushi</td>\n",
       "      <td>1949</td>\n",
       "      <td>1982</td>\n",
       "      <td>actor</td>\n",
       "      <td>tt0078723,tt0080455,tt0072562,tt0077975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nm0000005</td>\n",
       "      <td>Ingmar Bergman</td>\n",
       "      <td>1918</td>\n",
       "      <td>2007</td>\n",
       "      <td>actor</td>\n",
       "      <td>tt0050986,tt0083922,tt0069467,tt0050976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      nconst      primaryName birthYear deathYear primaryProfession  \\\n",
       "0  nm0000001     Fred Astaire      1899      1987             actor   \n",
       "1  nm0000002    Lauren Bacall      1924      2014           actress   \n",
       "2  nm0000003  Brigitte Bardot      1934        \\N           actress   \n",
       "3  nm0000004     John Belushi      1949      1982             actor   \n",
       "4  nm0000005   Ingmar Bergman      1918      2007             actor   \n",
       "\n",
       "                            knownForTitles  \n",
       "0  tt0050419,tt0053137,tt0043044,tt0072308  \n",
       "1  tt0117057,tt0037382,tt0071877,tt0038355  \n",
       "2  tt0049189,tt0059956,tt0054452,tt0057345  \n",
       "3  tt0078723,tt0080455,tt0072562,tt0077975  \n",
       "4  tt0050986,tt0083922,tt0069467,tt0050976  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.exists('./pickles/filtered_actors.pkl'):\n",
    "    print(\"Reading from pickle...\")\n",
    "    actors = pd.read_pickle(\"./pickles/filtered_actors.pkl\")\n",
    "else:\n",
    "    people_df = pd.read_csv(\n",
    "        './data/name.basics.tsv.gz',\n",
    "        sep='\\t',\n",
    "        converters={'primaryProfession': split_profession}\n",
    "    )\n",
    "    people_prof_exploded = people_df.explode('primaryProfession')\n",
    "    actors = people_prof_exploded[\n",
    "        (people_prof_exploded.primaryProfession == 'actress') | (people_prof_exploded.primaryProfession == 'actor')]\n",
    "    actors.to_pickle(\"./pickles/filtered_actors.pkl\")\n",
    "actors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3626035\n"
     ]
    }
   ],
   "source": [
    "print(len(actors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(session, url):\n",
    "    nconst=url.rsplit('/', 2)[-2]\n",
    "    result = {\n",
    "        'nconst': nconst,\n",
    "        'year': None,\n",
    "        'category': None,\n",
    "        'w_n': None,\n",
    "        'description': None,\n",
    "        'movie': None,\n",
    "        'tconst': None \n",
    "    }\n",
    "    \n",
    "    req = session.get(url)\n",
    "    req.raise_for_status()    \n",
    "    \n",
    "    body=req.text\n",
    "    \n",
    "    soup=BeautifulSoup(body,'html.parser')\n",
    "    awards=soup.find_all('tr')\n",
    "    if awards is not None:\n",
    "        for award in awards:\n",
    "            if award.find('td', class_='award_year') is not None:\n",
    "                if award.find('td', class_='award_year').find('a') is not None:\n",
    "                    year = award.find('td', class_='award_year').find('a').text\n",
    "                    if year is not None:\n",
    "                        try:\n",
    "                            year = int(year.replace(\"\\n\", \"\").strip())\n",
    "                            result['year'] = year\n",
    "                        except:\n",
    "                            print(\"Failed to parse int year {0}\".format(year))\n",
    "                        \n",
    "                    \n",
    "    \n",
    "            if award.find('span',class_='award_category') is not None:\n",
    "                result['category'] = award.find('span',class_='award_category').text\n",
    "        \n",
    "            if award.find('td',class_=\"award_outcome\") is not None:\n",
    "                outcome = award.find('td',class_=\"award_outcome\").find('b')\n",
    "                if outcome is not None:\n",
    "                    outcome_txt = outcome.text\n",
    "                    if outcome_txt is not None:\n",
    "                        result['w_n'] = outcome_txt.replace(\"\\n\", \"\").strip()\n",
    "        \n",
    "            if award.find('td', class_='award_description') is not None:\n",
    "                award_txt = award.find('td',class_='award_description').find(text=True, recursive=False)\n",
    "                award_info = award.find('td',class_='award_description').find('a')\n",
    "                if award_txt is not None:\n",
    "                    result['description'] = award_txt.replace(\"\\n\", \"\").strip()\n",
    "                \n",
    "                if award_info is not None:\n",
    "                    result['movie'] = award_info.text\n",
    "                    tconst_res = re.search('tt\\d{7}', award_info.get('href'))\n",
    "                    if tconst_res is not None:\n",
    "                        result['tconst'] = tconst_res.group(0)\n",
    "        \n",
    "        return result\n",
    "    else:\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 1 of 3 completed with IP 185.220.100.252. Took 35.22462797164917 seconds.\n",
      "Wrote pickle ./pickles/actortor/1_0-600_actors_tor.pkl with 600 rows\n",
      "chunk 2 of 3 completed with IP 5.199.135.107. Took 26.74442481994629 seconds.\n",
      "Wrote pickle ./pickles/actortor/2_600-1200_actors_tor.pkl with 600 rows\n",
      "chunk 3 of 3 completed with IP 195.176.3.19. Took 5.84783411026001 seconds.\n",
      "Wrote pickle ./pickles/actortor/3_1200-1800_actors_tor.pkl with 100 rows\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "!!!**********************************!!!\n",
      "\n",
      "Scraping succeeded!\n",
      "\n",
      "!!!**********************************!!!\n",
      "Note: Next, run the script that combine all pickles\n"
     ]
    }
   ],
   "source": [
    "# Set the start index and end index. Ip is renewed after each chunk of urls of 'step_size'. \n",
    "# After every chunk, result is written to a pickle. So if you need to stop the execution in the middle, note the\n",
    "# index range of the last successfully written pickle file (from the printed logs) and use the remaining range\n",
    "# for start and end index to resume from where you stopped.\n",
    "postfix = 'actors_tor'\n",
    "s_idx = 1211100\n",
    "e_idx = 1212400\n",
    "step_size = 600\n",
    "# [0:3626035]\n",
    "\n",
    "base_url = 'https://www.imdb.com/name/{0}/awards'\n",
    "urls=[]\n",
    "for index, row in actors[s_idx:e_idx].iterrows():\n",
    "    urls.append(base_url.format(row['nconst']))\n",
    "\n",
    "used_ips = []\n",
    "failed_urls = []\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=150) as executor:\n",
    "    session = get_tor_session()\n",
    "    chunk_indeces = np.arange(0, len(urls), step_size)\n",
    "    \n",
    "    for idx, start in enumerate(chunk_indeces, start=1):\n",
    "        start_t = time.time()\n",
    "        finals = []\n",
    "        renew_connection()\n",
    "        session = get_tor_session()\n",
    "        new_ip = ast.literal_eval(session.get(\"http://httpbin.org/ip\").text)[\"origin\"].split(\",\")[0]\n",
    "        while new_ip in used_ips:\n",
    "            print(\"Renewed IP {0} already used. Waiting 5s to renew...\".format(new_ip))\n",
    "            time.sleep(5)\n",
    "            renew_connection()\n",
    "            session = get_tor_session()\n",
    "            new_ip = ast.literal_eval(session.get(\"http://httpbin.org/ip\").text)[\"origin\"].split(\",\")[0]\n",
    "        used_ips.append(new_ip)\n",
    "        \n",
    "        url_chunk = urls[start:start + step_size] + failed_urls\n",
    "        failed_urls = []\n",
    "        \n",
    "        future_to_url = {executor.submit(get_data, session, url): url for url in url_chunk}\n",
    "        for future in concurrent.futures.as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                data = future.result()\n",
    "                finals.append(data)\n",
    "            except Exception as exc:\n",
    "                failed_urls.append(url.strip(\"'\"))\n",
    "                print('%s generated an exception: %s. Added to retry list.' % (url, exc))\n",
    "        \n",
    "        concurrent.futures.wait(\n",
    "            list(future_to_url.keys()), \n",
    "            timeout=None, \n",
    "            return_when=concurrent.futures.ALL_COMPLETED)\n",
    "        end_t = time.time()\n",
    "        print(\"chunk {0} of {1} completed with IP {2}. Took {3} seconds.\"\\\n",
    "              .format(idx, len(chunk_indeces), new_ip, end_t - start_t))   \n",
    "        actors_df = pd.DataFrame(data=finals)\n",
    "        pickle_path = \"./pickles/actortor/{0}_{1}-{2}_{3}.pkl\"\\\n",
    "                        .format(idx, start, start + step_size, postfix)\n",
    "        actors_df.to_pickle(pickle_path)\n",
    "        print(\"Wrote pickle {0} with {1} rows\".format(pickle_path, len(actors_df)))\n",
    "        \n",
    "    if len(failed_urls) > 0:\n",
    "        print('\\n\\n\\n\\n!!!**********************************!!!')\n",
    "        print('Failed to retrieve the following URLs.\\n')\n",
    "        print(failed_urls)\n",
    "        print('\\n!!!**********************************!!!')\n",
    "        print('Note: Run the following cell to retry the failed urls')\n",
    "    else:\n",
    "        print(\"\\n\\n\\n\\n!!!**********************************!!!\\n\")\n",
    "        print(\"Scraping succeeded!\")\n",
    "        print('\\n!!!**********************************!!!')\n",
    "        print('''Note: Next, run the script that combine all pickles''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 1 of 1 completed with IP 104.244.78.102. Took 1.212857961654663 seconds.\n",
      "Wrote pickle ./pickles/actortor/1_0-10_actors_tor_retry.pkl with 1 rows\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "!!!**********************************!!!\n",
      "\n",
      "Retry succeeded!\n",
      "\n",
      "!!!**********************************!!!\n",
      "Note: Run the following script to combine all pickles\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Run the following script only if there were any failed URLS at the end of the previous script execution.\n",
    "Run this several times if you still see failures until there are none. But if some dont succeed even \n",
    "after multiple attempts, its probably a problem on imdb side and we can just ignore them.\n",
    "'''\n",
    "# urls = failed_urls\n",
    "\n",
    "# if len(urls) > 0:\n",
    "#     retry_failed_urls = []\n",
    "\n",
    "#     with concurrent.futures.ThreadPoolExecutor(max_workers=150) as executor:\n",
    "#         session = get_tor_session()\n",
    "#         chunk_indeces = np.arange(0, len(urls), step_size)\n",
    "\n",
    "#         for idx, start in enumerate(chunk_indeces, start=1):\n",
    "#             start_t = time.time()\n",
    "#             finals = []\n",
    "#             renew_connection()\n",
    "#             session = get_tor_session()\n",
    "#             new_ip = ast.literal_eval(session.get(\"http://httpbin.org/ip\").text)[\"origin\"].split(\",\")[0]\n",
    "#             while new_ip in used_ips:\n",
    "#                 print(\"Renewed IP {0} already used. Waiting 5s to renew...\".format(new_ip))\n",
    "#                 time.sleep(5)\n",
    "#                 renew_connection()\n",
    "#                 session = get_tor_session()\n",
    "#                 new_ip = ast.literal_eval(session.get(\"http://httpbin.org/ip\").text)[\"origin\"].split(\",\")[0]\n",
    "#             used_ips.append(new_ip)\n",
    "\n",
    "#             url_chunk = urls[start:start + step_size] + retry_failed_urls\n",
    "#             retry_failed_urls = []\n",
    "\n",
    "#             future_to_url = {executor.submit(get_data, session, url): url for url in url_chunk}\n",
    "#             for future in concurrent.futures.as_completed(future_to_url):\n",
    "#                 url = future_to_url[future]\n",
    "#                 try:\n",
    "#                     data = future.result()\n",
    "#                     finals.append(data)\n",
    "#                 except Exception as exc:\n",
    "#                     retry_failed_urls.append(url.strip(\"'\"))\n",
    "#                     print('%s generated an exception: %s. Added to retry list.' % (url, exc))\n",
    "\n",
    "#             concurrent.futures.wait(list(future_to_url.keys()), timeout=None, return_when=concurrent.futures.ALL_COMPLETED)\n",
    "#             end_t = time.time()\n",
    "#             print(\"chunk {0} of {1} completed with IP {2}. Took {3} seconds.\".format(idx, len(chunk_indeces), new_ip, end_t - start_t))   \n",
    "#             actors_df = pd.DataFrame(data=finals)\n",
    "#             if len(actors_df) > 0:\n",
    "#                 pickle_path = \"./pickles/actortor/{0}_{1}-{2}_{3}_retry.pkl\".format(idx, start, start + step_size, postfix)\n",
    "#                 actors_df.to_pickle(pickle_path)\n",
    "#                 print(\"Wrote pickle {0} with {1} rows\".format(pickle_path, len(actors_df)))\n",
    "\n",
    "#         if len(retry_failed_urls) > 0:\n",
    "#             print('\\n\\n\\n\\n!!!**********************************!!!')\n",
    "#             print('Retry failed with the following URLs.\\n')\n",
    "#             print(retry_failed_urls)\n",
    "#             print('\\n!!!**********************************!!!')\n",
    "#             print('Note: Run this script several times to see if all succeed.',\n",
    "#             'If some dont, it probably means they wont ever. so just give them up.')\n",
    "#         else:\n",
    "#             print(\"\\n\\n\\n\\n!!!**********************************!!!\\n\")\n",
    "#             print(\"Retry succeeded!\")\n",
    "#             print('\\n!!!**********************************!!!')\n",
    "#             print('''Note: Run the following script to combine all pickles''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine the created pickles into one Dataframe\n",
    "\n",
    "dir_path = \"./pickles/actortor/\"\n",
    "# Ignore any sytem files starting with . or folders if any\n",
    "pickles = [f for f in listdir(dir_path) if isfile(join(dir_path, f)) and not f.startswith('.')]\n",
    "\n",
    "dfs = list(map(lambda x: pd.read_pickle(\"{0}/{1}\".format(dir_path,x)), pickles))\n",
    "combined_df = pd.concat(dfs)\n",
    "combined_df.reset_index(drop=True, inplace=True)\n",
    "combined_df.to_pickle(\"./pickles/complete_actor_{0}.pkl\".format(e_idx))\n",
    "len(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
