{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping actors awards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import sys\n",
    "import time\n",
    "import requests\n",
    "import traceback\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "import urllib.request\n",
    "import concurrent.futures\n",
    "from os import listdir\n",
    "from stem import Signal\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from os.path import isfile, join\n",
    "from stem.control import Controller\n",
    "\n",
    "old_stdout = sys.stdout\n",
    "\n",
    "log_file = open(\"actor_scrape.log\",\"w\")\n",
    "\n",
    "sys.stdout = log_file\n",
    "\n",
    "print(\"Initial test log\")\n",
    "\n",
    "pickles_save_dir = \"./pickles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tor_session():\n",
    "    session = requests.session()\n",
    "    # Tor uses the 9050 port as the default socks port\n",
    "    session.proxies = {'http':  'socks5://127.0.0.1:9050',\n",
    "                       'https': 'socks5://127.0.0.1:9050'}\n",
    "    return session\n",
    "\n",
    "def renew_connection():\n",
    "    with Controller.from_port(port = 9051) as controller:\n",
    "        controller.authenticate(password=\"password\")\n",
    "        controller.signal(Signal.NEWNYM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_pickle(\"./pickles/complete_movies.pkl\")\n",
    "len(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stars_nonempty = movies[movies.astype(str)['stars'] != '[]']\n",
    "stars = stars_nonempty.explode('stars')[['stars']]\n",
    "stars.drop_duplicates(subset='stars', keep='first', inplace=True)\n",
    "stars.reset_index(drop=True, inplace=True)\n",
    "stars.rename(columns={\"stars\": \"nconst\"}, inplace=True)\n",
    "display(stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actors = stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(session, url):\n",
    "    nconst=url.rsplit('/', 2)[-2]\n",
    "    result = []\n",
    "    \n",
    "    req = session.get(url)\n",
    "    req.raise_for_status()    \n",
    "\n",
    "    body=req.text\n",
    "\n",
    "    soup=BeautifulSoup(body,'html.parser')\n",
    "    awards=soup.find_all('tr')\n",
    "    if awards is not None:\n",
    "        year_buf = []\n",
    "        w_n_buf = []\n",
    "        category_buf = []\n",
    "        for award in awards:\n",
    "            award_record = {\n",
    "                'nconst': nconst,\n",
    "                'year': None,\n",
    "                'category': None,\n",
    "                'w_n': None,\n",
    "                'description': None,\n",
    "                'movie': None,\n",
    "                'tconst': None\n",
    "            } \n",
    "            if award.find('td', class_='award_year') is not None:\n",
    "                award_year_td = award.find('td', class_='award_year')\n",
    "                if award_year_td.find('a') is not None:\n",
    "\n",
    "                    year = award.find('td', class_='award_year').find('a').text\n",
    "                    if year is not None:\n",
    "                        try:\n",
    "                            year = int(year.replace(\"\\n\", \"\").strip())\n",
    "                            if year:\n",
    "                                award_record['year'] = year\n",
    "\n",
    "                            # If the td has a row span more than 1, cache the value so it will be used \n",
    "                            # with the subsequent corresponding trs as well\n",
    "                            if award_year_td['rowspan'] is not None:\n",
    "                                award_year_td_rs = int(award_year_td['rowspan'])\n",
    "                                for i in range(0, award_year_td_rs - 1):\n",
    "                                    year_buf.append(year)\n",
    "                        except:\n",
    "                            print(\"Failed to parse int year {0}\".format(year))\n",
    "\n",
    "            elif len(year_buf) > 0:\n",
    "                buffed_year = year_buf.pop()\n",
    "                if buffed_year:\n",
    "                    award_record['year'] = buffed_year\n",
    "\n",
    "            if award.find('td',class_='award_outcome') is not None:\n",
    "                award_outcome_td = award.find('td',class_='award_outcome')\n",
    "                if award_outcome_td.find('span',class_='award_category') is not None:\n",
    "                    award_cat = award_outcome_td.find('span',class_='award_category').text\n",
    "                    if award_cat:\n",
    "                        award_record['category'] = award_cat\n",
    "\n",
    "                if award_outcome_td.find('b') is not None:\n",
    "                    w_n_txt = award_outcome_td.find('b').text\n",
    "                    if w_n_txt is not None:\n",
    "                        w_n = w_n_txt.replace(\"\\n\", \"\").strip()\n",
    "                        if w_n:\n",
    "                            award_record['w_n'] = w_n\n",
    "\n",
    "                if award_outcome_td['rowspan'] is not None:\n",
    "                    award_outcome_td_rs = int(award_outcome_td['rowspan'])\n",
    "                    for i in range(0, award_outcome_td_rs - 1):\n",
    "                        category_buf.append(award_cat)\n",
    "                        w_n_buf.append(w_n)\n",
    "            else:\n",
    "                if len(w_n_buf) > 0:\n",
    "                    buffed_w_n = w_n_buf.pop()\n",
    "                    if buffed_w_n:\n",
    "                        award_record['w_n'] = buffed_w_n\n",
    "                if len(category_buf) > 0:\n",
    "                    buffed_category = category_buf.pop()\n",
    "                    if buffed_category:\n",
    "                        award_record['category'] = buffed_category\n",
    "\n",
    "\n",
    "            if award.find('td', class_='award_description') is not None:\n",
    "                award_txt = award.find('td',class_='award_description').find(text=True, recursive=False)\n",
    "                award_info = award.find('td',class_='award_description').find('a', href=re.compile(r'.*tt\\d{7,8}.*'))\n",
    "                if award_txt is not None:\n",
    "                    desc_txt = award_txt.replace(\"\\n\", \"\").strip()\n",
    "                    if desc_txt:\n",
    "                        award_record['description'] = desc_txt\n",
    "\n",
    "                if award_info is not None:\n",
    "                    tconst_res = re.search('tt\\d{7,8}', award_info.get('href'))\n",
    "                    if tconst_res is not None:\n",
    "                        tconst_val = tconst_res.group(0)\n",
    "                        if tconst_val:\n",
    "                            award_record['tconst'] = tconst_val\n",
    "                        award_info_txt = award_info.text\n",
    "                        if award_info_txt:\n",
    "                            award_record['movie'] = award_info.text\n",
    "            result.append(award_record)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the start index and end index. Ip is renewed after each chunk of urls of 'step_size'. \n",
    "# After every chunk, result is written to a pickle. So if you need to stop the execution in the middle, note the\n",
    "# index range of the last successfully written pickle file (from the printed logs) and use the remaining range\n",
    "# for start and end index to resume from where you stopped.\n",
    "postfix = 'stars'\n",
    "s_idx = 0\n",
    "e_idx = len(actors)\n",
    "step_size = 4000\n",
    "\n",
    "print(\"Starting scraping...\")\n",
    "\n",
    "base_url = 'https://www.imdb.com/name/{0}/awards'\n",
    "urls=[]\n",
    "for index, row in actors[s_idx:e_idx].iterrows():\n",
    "    urls.append(base_url.format(row['nconst']))\n",
    "\n",
    "used_ips = []\n",
    "failed_urls = []\n",
    "dropped_urls = []\n",
    "\n",
    "retry_counts = collections.defaultdict(int)\n",
    "\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=100) as executor:\n",
    "    session = get_tor_session()\n",
    "    chunk_indeces = np.arange(0, len(urls), step_size)\n",
    "    \n",
    "    for idx, start in enumerate(chunk_indeces, start=1):\n",
    "        start_t = time.time()\n",
    "        finals = []\n",
    "        renew_connection()\n",
    "        session = get_tor_session()\n",
    "        new_ip = ast.literal_eval(session.get(\"http://httpbin.org/ip\").text)[\"origin\"].split(\",\")[0]\n",
    "        while new_ip in used_ips:\n",
    "            print(\"Renewed IP {0} already used. Waiting 5s to renew...\".format(new_ip))\n",
    "            time.sleep(5)\n",
    "            renew_connection()\n",
    "            session = get_tor_session()\n",
    "            new_ip = ast.literal_eval(session.get(\"http://httpbin.org/ip\").text)[\"origin\"].split(\",\")[0]\n",
    "        used_ips.append(new_ip)\n",
    "        \n",
    "        url_chunk = urls[start:start + step_size] + failed_urls\n",
    "        failed_urls = []\n",
    "        \n",
    "        print(\"Submitting {0} URLs to the executor\".format(len(url_chunk)))\n",
    "        \n",
    "        future_to_url = {executor.submit(get_data, session, url): url for url in url_chunk}\n",
    "        for future in concurrent.futures.as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                data = future.result()\n",
    "                finals += data\n",
    "            except Exception as exc:\n",
    "                stripped = url.strip(\"'\")\n",
    "                # Retry only once\n",
    "                if retry_counts[stripped] == 0:\n",
    "                    failed_urls.append(stripped)\n",
    "                    retry_counts[stripped] = 1\n",
    "                    print('%s generated an exception: %s. Added to retry list.' % (url, exc))\n",
    "                else:\n",
    "                    print('%s generated an exception: %s. Dropping since retry failed.' % (url, exc))\n",
    "                    dropped_urls.append(stripped)\n",
    "\n",
    "        \n",
    "        concurrent.futures.wait(\n",
    "            list(future_to_url.keys()), \n",
    "            timeout=None, \n",
    "            return_when=concurrent.futures.ALL_COMPLETED)\n",
    "        end_t = time.time()\n",
    "        print(\"chunk {0} of {1} completed with IP {2}. Took {3} seconds.\"\\\n",
    "              .format(idx, len(chunk_indeces), new_ip, end_t - start_t))   \n",
    "        actors_df = pd.DataFrame(data=finals)\n",
    "        acror_pickle_save_dir = './pickles/actor'\n",
    "        if not os.path.exists(acror_pickle_save_dir):\n",
    "            os.makedirs(acror_pickle_save_dir)\n",
    "        pickle_path = \"{4}/{0}_{1}-{2}_{3}.pkl\"\\\n",
    "                        .format(idx, start, start + step_size, postfix, acror_pickle_save_dir)\n",
    "        actors_df.to_pickle(pickle_path)\n",
    "        print(\"Wrote pickle {0} with {1} rows\".format(pickle_path, len(actors_df)))\n",
    "        \n",
    "    if len(failed_urls) > 0:\n",
    "        print('\\n\\n\\n\\n!!!**********************************!!!')\n",
    "        print('Failed to retrieve the following URLs.\\n')\n",
    "        print(failed_urls)\n",
    "        print('\\n!!!**********************************!!!')\n",
    "        print('Note: Run the following cell to retry the failed urls')\n",
    "    else:\n",
    "        print(\"\\n\\n\\n\\n!!!**********************************!!!\\n\")\n",
    "        print(\"Scraping succeeded!\")\n",
    "        print('\\n!!!**********************************!!!')\n",
    "        print('''Note: Next, run the script that combine all pickles''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Run the following script only if there were any failed URLS at the end of the previous script execution.\n",
    "Run this several times if you still see failures until there are none. But if some dont succeed even \n",
    "after multiple attempts, its probably a problem on imdb side and we can just ignore them.\n",
    "'''\n",
    "urls = failed_urls\n",
    "if len(urls) > 0:\n",
    "    retry_failed_urls = []\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=150) as executor:\n",
    "    session = get_tor_session(None)\n",
    "    chunk_indeces = np.arange(0, len(urls), step_size)\n",
    "\n",
    "    for idx, start in enumerate(chunk_indeces, start=1):\n",
    "        start_t = time.time()\n",
    "        finals = []\n",
    "        renew_connection()\n",
    "        session = get_tor_session(session)\n",
    "        new_ip = ast.literal_eval(session.get(\"http://httpbin.org/ip\").text)[\"origin\"].split(\",\")[0]\n",
    "        while new_ip in used_ips:\n",
    "            print(\"Renewed IP {0} already used. Waiting 5s to renew...\".format(new_ip))\n",
    "            time.sleep(5)\n",
    "            renew_connection()\n",
    "            session = get_tor_session(session)\n",
    "            new_ip = ast.literal_eval(session.get(\"http://httpbin.org/ip\").text)[\"origin\"].split(\",\")[0]\n",
    "        used_ips.append(new_ip)\n",
    "\n",
    "        url_chunk = urls[start:start + step_size] + retry_failed_urls\n",
    "        retry_failed_urls = []\n",
    "\n",
    "        future_to_url = {executor.submit(get_data, session, url): url for url in url_chunk}\n",
    "        for future in concurrent.futures.as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                data = future.result()\n",
    "                finals += data\n",
    "            except Exception as exc:\n",
    "                retry_failed_urls.append(url.strip(\"'\"))\n",
    "                print('%s generated an exception: %s. Added to retry list.' % (url, exc))\n",
    "\n",
    "        concurrent.futures.wait(list(future_to_url.keys()), timeout=None, return_when=concurrent.futures.ALL_COMPLETED)\n",
    "        end_t = time.time()\n",
    "        print(\"chunk {0} of {1} completed with IP {2}. Took {3} seconds.\".format(idx, len(chunk_indeces), new_ip, end_t - start_t))   \n",
    "        actors_df = pd.DataFrame(data=finals)\n",
    "        if len(actors_df) > 0:\n",
    "            pickle_path = \"./pickles/actor/{0}_{1}-{2}_{3}_retry.pkl\".format(idx, start, start + step_size, postfix)\n",
    "            actors_df.to_pickle(pickle_path)\n",
    "            print(\"Wrote pickle {0} with {1} rows\".format(pickle_path, len(actors_df)))\n",
    "\n",
    "    if len(retry_failed_urls) > 0:\n",
    "        print('\\n\\n\\n\\n!!!**********************************!!!')\n",
    "        print('Retry failed with the following URLs.\\n')\n",
    "        print(retry_failed_urls)\n",
    "        print('\\n!!!**********************************!!!')\n",
    "        print('Note: Run this script several times to see if all succeed.',\n",
    "        'If some dont, it probably means they wont ever. so just give them up.')\n",
    "    else:\n",
    "        print(\"\\n\\n\\n\\n!!!**********************************!!!\\n\")\n",
    "        print(\"Retry succeeded!\")\n",
    "        print('\\n!!!**********************************!!!')\n",
    "        print('''Note: Run the following script to combine all pickles''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the created pickles into one Dataframe\n",
    "\n",
    "dir_path = \"./pickles/actor/\"\n",
    "# Ignore any sytem files starting with . or folders if any\n",
    "pickles = [f for f in listdir(dir_path) if isfile(join(dir_path, f)) and not f.startswith('.')]\n",
    "\n",
    "dfs = list(map(lambda x: pd.read_pickle(\"{0}/{1}\".format(dir_path,x)), pickles))\n",
    "combined_df = pd.concat(dfs)\n",
    "combined_df.reset_index(drop=True, inplace=True)\n",
    "combined_df.to_pickle(\"./pickles/complete_actors.pkl\".format(e_idx))\n",
    "len(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie cast scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import sys\n",
    "import time\n",
    "import requests\n",
    "import traceback\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "import urllib.request\n",
    "import concurrent.futures\n",
    "from os import listdir\n",
    "from stem import Signal\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from os.path import isfile, join\n",
    "from stem.control import Controller\n",
    "\n",
    "pickles_save_dir = \"./pickles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tor_session():\n",
    "    session = requests.session()\n",
    "    # Tor uses the 9050 port as the default socks port\n",
    "    session.proxies = {'http':  'socks5://127.0.0.1:9050',\n",
    "                       'https': 'socks5://127.0.0.1:9050'}\n",
    "    return session\n",
    "\n",
    "def renew_connection():\n",
    "    with Controller.from_port(port = 9051) as controller:\n",
    "        controller.authenticate(password=\"password\")\n",
    "        controller.signal(Signal.NEWNYM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('./pickles/filtered_movies.pkl'):\n",
    "    movies = pd.read_pickle(\"./pickles/filtered_movies.pkl\")\n",
    "else:\n",
    "    title_basics = pd.read_csv(\"data/title.basics.tsv.gz\", sep='\\t')\n",
    "    movies = title_basics[title_basics.titleType == 'movie']\n",
    "    if not os.path.exists(pickles_save_dir):\n",
    "        os.makedirs(pickles_save_dir)\n",
    "    movies.to_pickle(\"{0}/filtered_movies.pkl\".format(pickles_save_dir))\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(session, url):\n",
    "    info=[]\n",
    "    \n",
    "    r = session.get(url)\n",
    "    r.raise_for_status()\n",
    "    \n",
    "    page_body = r.text\n",
    "    soup = BeautifulSoup(page_body, 'html.parser')\n",
    "    \n",
    "    #tconst\n",
    "    tconst=url.rsplit('/', 2)[-2]\n",
    "    \n",
    "    scraped_data = {\n",
    "        \"tconst\": tconst,\n",
    "        \"cast\": []\n",
    "    }\n",
    "    \n",
    "    table = soup.find('table', class_='cast_list')\n",
    "    if table is not None:\n",
    "        atags = table.find_all('a')\n",
    "        if atags is not None:\n",
    "            for atag in atags:\n",
    "                nconst_res = re.search('nm\\d{7,8}', atag.get('href'))\n",
    "                if nconst_res is not None:\n",
    "                    scraped_data['cast'].append(nconst_res.group(0))\n",
    "            \n",
    "    # Drop duplicates\n",
    "    scraped_data['cast'] = list(dict.fromkeys(scraped_data['cast']))\n",
    "    scraped_data['count'] = len(scraped_data['cast'])\n",
    "    return scraped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the start index and end index. Ip is renewed after each chunk of urls of 'step_size'. \n",
    "# After every chunk, result is written to a pickle. So if you need to stop the execution in the middle, note the\n",
    "# index range of the last successfully written pickle file (from the printed logs) and use the remaining range\n",
    "# for start and end index to resume from where you stopped.\n",
    "postfix = 'tor'\n",
    "s_idx = 0\n",
    "e_idx = len(movies)\n",
    "step_size = 4000\n",
    "\n",
    "base_url = 'https://www.imdb.com/title/{0}/fullcredits'\n",
    "urls=[]\n",
    "for index, row in movies[s_idx:e_idx].iterrows():\n",
    "    urls.append(base_url.format(row['tconst']))\n",
    "\n",
    "used_ips = []\n",
    "failed_urls = []\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=100) as executor:\n",
    "    session = get_tor_session()\n",
    "    chunk_indeces = np.arange(0, len(urls), step_size)\n",
    "    \n",
    "    for idx, start in enumerate(chunk_indeces, start=1):\n",
    "        start_t = time.time()\n",
    "        finals = []\n",
    "        renew_connection()\n",
    "        session = get_tor_session()\n",
    "        new_ip = ast.literal_eval(session.get(\"http://httpbin.org/ip\").text)[\"origin\"].split(\",\")[0]\n",
    "        while new_ip in used_ips:\n",
    "            print(\"Renewed IP {0} already used. Waiting 5s to renew...\".format(new_ip))\n",
    "            time.sleep(5)\n",
    "            renew_connection()\n",
    "            session = get_tor_session()\n",
    "            new_ip = ast.literal_eval(session.get(\"http://httpbin.org/ip\").text)[\"origin\"].split(\",\")[0]\n",
    "        used_ips.append(new_ip)\n",
    "        \n",
    "        url_chunk = urls[start:start + step_size] + failed_urls\n",
    "        failed_urls = []\n",
    "        \n",
    "        future_to_url = {executor.submit(get_data, session, url): url for url in url_chunk}\n",
    "        for future in concurrent.futures.as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                data = future.result()\n",
    "                finals.append(data)\n",
    "            except Exception as exc:\n",
    "                failed_urls.append(url.strip(\"'\"))\n",
    "                print('%s generated an exception: %s. Added to retry list.' % (url, exc))\n",
    "                exc_info = sys.exc_info()\n",
    "                traceback.print_exception(*exc_info)\n",
    "        \n",
    "        concurrent.futures.wait(\n",
    "            list(future_to_url.keys()), \n",
    "            timeout=None, \n",
    "            return_when=concurrent.futures.ALL_COMPLETED)\n",
    "        end_t = time.time()\n",
    "        print(\"chunk {0} of {1} completed with IP {2}. Took {3} seconds.\"\\\n",
    "              .format(idx, len(chunk_indeces), new_ip, end_t - start_t))   \n",
    "        cast_df = pd.DataFrame(data=finals)\n",
    "        cast_pickles_savedir = \"{0}/castretries/nullrecheck\".format(pickles_save_dir)\n",
    "        if not os.path.exists(cast_pickles_savedir):\n",
    "            os.makedirs(cast_pickles_savedir)\n",
    "        pickle_path = \"{4}/{0}_{1}-{2}_{3}.pkl\".format(idx, start, start + step_size, postfix, cast_pickles_savedir)\n",
    "        cast_df.to_pickle(pickle_path)\n",
    "        print(\"Wrote pickle {0} with {1} rows\".format(pickle_path, len(cast_df)))\n",
    "                \n",
    "if len(failed_urls) > 0:\n",
    "    print('\\n\\n\\n\\n!!!**********************************!!!')\n",
    "    print('Failed to retrieve the following URLs.\\n')\n",
    "    print(failed_urls)\n",
    "    print('\\n!!!**********************************!!!')\n",
    "    print('Note: Run the following cell to retry the failed urls')\n",
    "else:\n",
    "    print(\"\\n\\n\\n\\n!!!**********************************!!!\\n\")\n",
    "    print(\"Scraping succeeded!\")\n",
    "    print('\\n!!!**********************************!!!')\n",
    "    print('''Note: Next, run the script that combine all pickles''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Run the following script only if there were any failed URLS at the end of the previous script execution.\n",
    "Run this several times if you still see failures until there are none. But if some dont succeed even \n",
    "after multiple attempts, its probably a problem on imdb side and we can just ignore them.\n",
    "'''\n",
    "urls = failed_urls\n",
    "\n",
    "if len(urls) > 0:\n",
    "    retry_failed_urls = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=150) as executor:\n",
    "        session = get_tor_session()\n",
    "        chunk_indeces = np.arange(0, len(urls), step_size)\n",
    "\n",
    "        for idx, start in enumerate(chunk_indeces, start=1):\n",
    "            start_t = time.time()\n",
    "            finals = []\n",
    "            renew_connection()\n",
    "            session = get_tor_session()\n",
    "            new_ip = ast.literal_eval(session.get(\"http://httpbin.org/ip\").text)[\"origin\"].split(\",\")[0]\n",
    "            while new_ip in used_ips:\n",
    "                print(\"Renewed IP {0} already used. Waiting 5s to renew...\".format(new_ip))\n",
    "                time.sleep(5)\n",
    "                renew_connection()\n",
    "                session = get_tor_session()\n",
    "                new_ip = ast.literal_eval(session.get(\"http://httpbin.org/ip\").text)[\"origin\"].split(\",\")[0]\n",
    "            used_ips.append(new_ip)\n",
    "\n",
    "            url_chunk = urls[start:start + step_size] + retry_failed_urls\n",
    "            retry_failed_urls = []\n",
    "\n",
    "            future_to_url = {executor.submit(get_data, session, url): url for url in url_chunk}\n",
    "            for future in concurrent.futures.as_completed(future_to_url):\n",
    "                url = future_to_url[future]\n",
    "                try:\n",
    "                    data = future.result()\n",
    "                    finals.append(data)\n",
    "                except Exception as exc:\n",
    "                    retry_failed_urls.append(url.strip(\"'\"))\n",
    "                    print('%s generated an exception: %s. Added to retry list.' % (url, exc))\n",
    "\n",
    "            concurrent.futures.wait(list(future_to_url.keys()), timeout=None, return_when=concurrent.futures.ALL_COMPLETED)\n",
    "            end_t = time.time()\n",
    "            print(\"chunk {0} of {1} completed with IP {2}. Took {3} seconds.\".format(idx, len(chunk_indeces), new_ip, end_t - start_t))   \n",
    "            cast_df = pd.DataFrame(data=finals)\n",
    "            if len(cast_df) > 0:\n",
    "                cast_pickles_savedir = \"{0}/castretries/nullrecheck\".format(pickles_save_dir)\n",
    "                if not os.path.exists(cast_pickles_savedir):\n",
    "                    os.makedirs(cast_pickles_savedir)\n",
    "                pickle_path = \"{4}/{0}_{1}-{2}_{3}_retry.pkl\".format(idx, start, start + step_size, postfix, cast_pickles_savedir)\n",
    "                cast_df.to_pickle(pickle_path)\n",
    "                print(\"Wrote pickle {0} with {1} rows\".format(pickle_path, len(cast_df)))\n",
    "\n",
    "        if len(retry_failed_urls) > 0:\n",
    "            print('\\n\\n\\n\\n!!!**********************************!!!')\n",
    "            print('Retry failed with the following URLs.\\n')\n",
    "            print(retry_failed_urls)\n",
    "            print('\\n!!!**********************************!!!')\n",
    "            print('Note: Run this script several times to see if all succeed.',\n",
    "            'If some dont, it probably means they wont ever. so just give them up.')\n",
    "        else:\n",
    "            print(\"\\n\\n\\n\\n!!!**********************************!!!\\n\")\n",
    "            print(\"Retry succeeded!\")\n",
    "            print('\\n!!!**********************************!!!')\n",
    "            print('''Note: Run the following script to combine all pickles''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the created pickles into one Dataframe\n",
    "\n",
    "dir_path = \"{0}/casttor\".format(pickles_save_dir)\n",
    "# Ignore any sytem files starting with . or folders if any\n",
    "pickles = [f for f in listdir(dir_path) if isfile(join(dir_path, f)) and not f.startswith('.')]\n",
    "dfs = list(map(lambda x: pd.read_pickle(\"{0}/{1}\".format(dir_path,x)), pickles))\n",
    "combined_df = pd.concat(dfs)\n",
    "combined_df.reset_index(drop=True, inplace=True)\n",
    "combined_df.to_pickle(\"./pickles/complete_cast{0}.pkl\".format(e_idx))\n",
    "len(combined_df)\n",
    "with pd.option_context('display.max_rows', 50000, 'display.max_columns', 20):\n",
    "    display(combined_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "import urllib.request\n",
    "import concurrent.futures\n",
    "from os import listdir\n",
    "from stem import Signal\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from os.path import isfile, join\n",
    "from stem.control import Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tor_session():\n",
    "    session = requests.session()\n",
    "    # Tor uses the 9050 port as the default socks port\n",
    "    session.proxies = {'http':  'socks5://127.0.0.1:9050',\n",
    "                       'https': 'socks5://127.0.0.1:9050'}\n",
    "    return session\n",
    "\n",
    "def renew_connection():\n",
    "    with Controller.from_port(port = 9051) as controller:\n",
    "        controller.authenticate(password=\"password\")\n",
    "        controller.signal(Signal.NEWNYM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('./pickles/filtered_movies.pkl'):\n",
    "    movies = pd.read_pickle(\"./pickles/filtered_movies.pkl\")\n",
    "else:\n",
    "    title_basics = pd.read_csv(\"data/title.basics.tsv\", sep='\\t')\n",
    "    movies = title_basics[title_basics.titleType == 'movie']\n",
    "    movies.to_pickle(\"./filtered_movies.pkl\")\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(session, url):\n",
    "    scraped_data = {\n",
    "    \"tconst\": [],\n",
    "    \"stars\": [],\n",
    "    \"oscarWins\": [],\n",
    "    \"nominations\": [],\n",
    "    \"wins\": [],\n",
    "    \"releaseDate\": [],\n",
    "    \"releaseCountry\": [],\n",
    "    \"plotKeywords\": [],\n",
    "    \"budget\": [],\n",
    "    \"worldwideGross\": [],\n",
    "    \"metascore\": [],\n",
    "    \"musicProducer\": []\n",
    "    }\n",
    "    \n",
    "    info=[]\n",
    "    \n",
    "    try:\n",
    "        r = session.get(url)\n",
    "        r.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as err:\n",
    "        print(err)\n",
    "    page_body = r.text\n",
    "    soup = BeautifulSoup(page_body, 'html.parser')\n",
    "    \n",
    "    #tconst\n",
    "    tconst=url.rsplit('/', 1)[-1]\n",
    "    scraped_data['tconst'].append(tconst)\n",
    "    \n",
    "        \n",
    "    # Stars\n",
    "    stars = []\n",
    "    stars_h4 = soup.find('h4', string='Stars:')\n",
    "    if stars_h4 is not None:\n",
    "        star_atags_parent = stars_h4.parent\n",
    "        if star_atags_parent is not None:\n",
    "            star_atags = star_atags_parent.find_all('a')\n",
    "            if star_atags is not None:\n",
    "                for atag in star_atags:\n",
    "                    if atag['href'].startswith('/name/'):\n",
    "                        stars.append(atag['href'].split('/')[2])\n",
    "    else:\n",
    "        stars_h4 = soup.find('h4', string='Star:')\n",
    "        if stars_h4 is not None:\n",
    "            star_atags_parent = stars_h4.parent\n",
    "            if star_atags_parent is not None:\n",
    "                star_atags = star_atags_parent.find_all('a')\n",
    "                if star_atags is not None:\n",
    "                    for atag in star_atags:\n",
    "                        if atag['href'].startswith('/name/'):\n",
    "                            stars.append(atag['href'].split('/')[2])\n",
    "    scraped_data['stars'].append(stars)\n",
    "    \n",
    "    \n",
    "    # Metascore\n",
    "    metascore = None\n",
    "    metascore_list = soup.select('.metacriticScore span:first-child')\n",
    "    if len(metascore_list) > 0:\n",
    "        metascore = metascore_list[0].string\n",
    "        \n",
    "    scraped_data['metascore'].append(metascore if metascore is None else str(metascore))\n",
    "    \n",
    "    \n",
    "    #awards\n",
    "    awrds_lines = soup.find_all(class_=\"awards-blurb\")\n",
    "    oscars = 0\n",
    "    wins = 0\n",
    "    nominations = 0\n",
    "    for line in awrds_lines:\n",
    "        \n",
    "        if line.findChild() is not None:\n",
    "            prepped_str = re.sub(' +', ' ', line.findChild().text.replace(\"\\n\", \" \").strip())\n",
    "            res = re.search('(W|w)on (\\d+) (O|o)scars.?', prepped_str)\n",
    "            if res is not None:\n",
    "                oscars = int(res.group(2))\n",
    "            \n",
    "        else:\n",
    "            prepped_str = re.sub(' +', ' ', line.text.replace(\"\\n\", \"\").strip())\n",
    "            \n",
    "            res = re.search('(\\d+) wins', prepped_str)\n",
    "            if res is not None:\n",
    "                wins = int(res.group(1))\n",
    "            \n",
    "            \n",
    "            res = re.search('(\\d+) nominations', prepped_str)\n",
    "            if res is not None:\n",
    "                nominations = int(res.group(1))\n",
    "    scraped_data['oscarWins'].append(oscars)\n",
    "    scraped_data['wins'].append(wins)\n",
    "    scraped_data['nominations'].append(nominations)\n",
    "    \n",
    "    \n",
    "    # Release date\n",
    "    release_date_h4 = soup.find('h4', string='Release Date:')\n",
    "    release_date = None\n",
    "    release_country = None\n",
    "    if release_date_h4 is not None:\n",
    "        release_date_raw_text = release_date_h4.parent.findAll(text=True, recursive=False)\n",
    "        release_date_prepped = re.sub(' +', ' ', ''.join(release_date_raw_text).replace(\"\\n\", \"\").strip())\n",
    "        date_str_match = re.search(r'\\d{1,2} \\w+ \\d{4}', release_date_prepped)\n",
    "        if date_str_match is not None:\n",
    "            release_date = datetime.strptime(date_str_match.group(), '%d %B %Y').date()\n",
    "        release_country_match = re.search(r'\\(([a-zA-Z ]{2,})\\)', release_date_prepped)\n",
    "        if release_country_match is not None and len(release_country_match.groups()) > 0:\n",
    "            release_country = release_country_match.group(1)\n",
    "        \n",
    "    scraped_data['releaseDate'].append(release_date)\n",
    "    scraped_data['releaseCountry'].append(release_country)\n",
    "    \n",
    "    \n",
    "    # Budget\n",
    "    budget_h4 = soup.find('h4', string='Budget:')\n",
    "    budget = None\n",
    "    if budget_h4 is not None:\n",
    "        budget_raw_text = budget_h4.parent.findAll(text=True, recursive=False)\n",
    "        budget = re.sub(' +', ' ', ''.join(budget_raw_text).replace(\"\\n\", \"\").strip())\n",
    "        \n",
    "    scraped_data['budget'].append(budget)\n",
    "    \n",
    "    \n",
    "    # worldwide gross\n",
    "    gross_h4 = soup.find('h4', string='Cumulative Worldwide Gross:')\n",
    "    gross = None\n",
    "    if gross_h4 is not None:\n",
    "        gross_h4_text = gross_h4.parent.findAll(text=True, recursive=False)\n",
    "        gross = re.sub(' +', ' ', ''.join(gross_h4_text).replace(\"\\n\", \"\").strip())\n",
    "    \n",
    "    scraped_data['worldwideGross'].append(gross)\n",
    "    \n",
    "    \n",
    "    # Plot keywords\n",
    "    keywords_verification_threshold = 2 # Consider only words atleast 2 people considered relavent\n",
    "    keywords_url = url + \"/keywords\"\n",
    "    r = get(keywords_url)\n",
    "    page_body = r.text\n",
    "    soup = BeautifulSoup(page_body, 'html.parser')\n",
    "    keywords = []\n",
    "    plot_keywords_items = soup.find_all(class_=\"soda sodavote\")\n",
    "    if plot_keywords_items is not None:\n",
    "        for plot_keywords_item in plot_keywords_items:\n",
    "            validity_text = plot_keywords_item.find(class_='interesting-count-text').a.text.strip()\n",
    "            validity_text_match = re.search(r'(\\d+) of', validity_text)\n",
    "            if validity_text_match is not None and len(validity_text_match.groups()) > 0:\n",
    "                if int(validity_text_match.group(1)) >= keywords_verification_threshold:\n",
    "                    keywords.append(plot_keywords_item.find(class_='sodatext').a.text.strip())\n",
    "    \n",
    "    scraped_data['plotKeywords'].append(keywords)\n",
    "    \n",
    "    \n",
    "    # Music producer\n",
    "    fullcredits_url = url + \"/fullcredits\"\n",
    "    r = get(fullcredits_url)\n",
    "    page_body = r.text\n",
    "    soup = BeautifulSoup(page_body, 'html.parser')\n",
    "    \n",
    "    music_producer = None\n",
    "    \n",
    "    full_credits_container = soup.find(id='fullcredits_content', class_='header')\n",
    "    if full_credits_container is not None:\n",
    "        full_credits = full_credits_container.find_all(recursive=False)\n",
    "        if full_credits is not None:\n",
    "            for idx, item in enumerate(full_credits, start=0):\n",
    "                if 'Music by' in item.text:\n",
    "                    producer_atag = full_credits[idx + 1].find('a')\n",
    "                    if producer_atag is not None:\n",
    "                        producer_href = producer_atag['href']\n",
    "                        if producer_href is not None:\n",
    "                            music_producer = producer_href.split('/')[2]\n",
    "                            break\n",
    "    \n",
    "    scraped_data['musicProducer'].append(music_producer)\n",
    "    return scraped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the start index and end index. Ip is renewed after each chunk of urls of 'step_size'. \n",
    "# After every chunk, result is written to a pickle. So if you need to stop the execution in the middle, note the\n",
    "# index range of the last successfully written pickle file (from the printed logs) and use the remaining range\n",
    "# for start and end index to resume from where you stopped.\n",
    "postfix = 'tor'\n",
    "s_idx = 0\n",
    "e_idx = 178118\n",
    "step_size = 4000\n",
    "\n",
    "base_url = 'https://www.imdb.com/title/'\n",
    "urls=[]\n",
    "for index, row in movies[s_idx:e_idx].iterrows():\n",
    "    urls.append(base_url + row['tconst'])\n",
    "\n",
    "used_ips = []\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=150) as executor:\n",
    "    session = get_tor_session()\n",
    "    chunk_indeces = np.arange(0, len(urls), step_size)\n",
    "    failed_urls = []\n",
    "    \n",
    "    for idx, start in enumerate(chunk_indeces, start=1):\n",
    "        start_t = time.time()\n",
    "        finals = {\n",
    "            \"tconst\": [],\n",
    "            \"stars\": [],\n",
    "            \"oscarWins\": [],\n",
    "            \"nominations\": [],\n",
    "            \"wins\": [],\n",
    "            \"releaseDate\": [],\n",
    "            \"releaseCountry\": [],\n",
    "            \"plotKeywords\": [],\n",
    "            \"budget\": [],\n",
    "            \"worldwideGross\": [],\n",
    "            \"metascore\": [],\n",
    "            \"musicProducer\": []\n",
    "        }\n",
    "        renew_connection()\n",
    "        session = get_tor_session()\n",
    "        new_ip = ast.literal_eval(session.get(\"http://httpbin.org/ip\").text)[\"origin\"].split(\",\")[0]\n",
    "        while new_ip in used_ips:\n",
    "            print(\"Renewed IP {0} already used. Waiting 5s to renew...\".format(new_ip))\n",
    "            time.sleep(5)\n",
    "            renew_connection()\n",
    "            session = get_tor_session()\n",
    "            new_ip = ast.literal_eval(session.get(\"http://httpbin.org/ip\").text)[\"origin\"].split(\",\")[0]\n",
    "        used_ips.append(new_ip)\n",
    "        \n",
    "        url_chunk = urls[start:start + step_size] + failed_urls\n",
    "        failed_urls = []\n",
    "        \n",
    "        future_to_url = {executor.submit(get_data, session, url): url for url in url_chunk}\n",
    "        for future in concurrent.futures.as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                data = future.result()\n",
    "                for k, v in data.items():\n",
    "                    finals[k].append(v[0])\n",
    "            except Exception as exc:\n",
    "                failed_urls.append(url.strip(\"'\"))\n",
    "                print('%s generated an exception: %s. Added to retry list.' % (url, exc))\n",
    "        \n",
    "        concurrent.futures.wait(list(future_to_url.keys()), timeout=None, return_when=concurrent.futures.ALL_COMPLETED)\n",
    "        end_t = time.time()\n",
    "        print(\"chunk {0} of {1} completed with IP {2}. Took {3} seconds.\".format(idx, len(chunk_indeces), new_ip, end_t - start_t))   \n",
    "        movies_df = pd.DataFrame(data=finals)\n",
    "        pickle_path = \"./pickles/tor/{0}_{1}-{2}_{3}.pkl\".format(idx, start, start + step_size, postfix)\n",
    "        movies_df.to_pickle(pickle_path)\n",
    "        print(\"Wrote pickle {0} with {1} rows\".format(pickle_path, len(movies_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the created pickles into one Dataframe\n",
    "\n",
    "dir_path = \"./pickles/tor\"\n",
    "# Ignore any sytem files starting with . or folders if any\n",
    "pickles = [f for f in listdir(dir_path) if isfile(join(dir_path, f)) and not f.startswith('.')]\n",
    "dfs = list(map(lambda x: pd.read_pickle(\"{0}/{1}\".format(dir_path,x)), pickles))\n",
    "combined_df = pd.concat(dfs)\n",
    "combined_df.reset_index(drop=True, inplace=True)\n",
    "combined_df.to_pickle(\"./pickles/complete_{0}.pkl\".format(e_idx))\n",
    "len(combined_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
