{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping actors awards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import sys\n",
    "import time\n",
    "import requests\n",
    "import traceback\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "import urllib.request\n",
    "import concurrent.futures\n",
    "from os import listdir\n",
    "from stem import Signal\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from os.path import isfile, join\n",
    "from stem.control import Controller\n",
    "\n",
    "old_stdout = sys.stdout\n",
    "\n",
    "log_file = open(\"actor_scrape.log\",\"w\")\n",
    "\n",
    "sys.stdout = log_file\n",
    "\n",
    "print(\"Initial test log\")\n",
    "\n",
    "pickles_save_dir = \"./pickles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tor_session():\n",
    "    session = requests.session()\n",
    "    # Tor uses the 9050 port as the default socks port\n",
    "    session.proxies = {'http':  'socks5://127.0.0.1:9050',\n",
    "                       'https': 'socks5://127.0.0.1:9050'}\n",
    "    return session\n",
    "\n",
    "def renew_connection():\n",
    "    with Controller.from_port(port = 9051) as controller:\n",
    "        controller.authenticate(password=\"password\")\n",
    "        controller.signal(Signal.NEWNYM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_pickle(\"./pickles/complete_movies.pkl\")\n",
    "len(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stars_nonempty = movies[movies.astype(str)['stars'] != '[]']\n",
    "stars = stars_nonempty.explode('stars')[['stars']]\n",
    "stars.drop_duplicates(subset='stars', keep='first', inplace=True)\n",
    "stars.reset_index(drop=True, inplace=True)\n",
    "stars.rename(columns={\"stars\": \"nconst\"}, inplace=True)\n",
    "display(stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actors = stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(session, url):\n",
    "    nconst=url.rsplit('/', 2)[-2]\n",
    "    result = []\n",
    "    \n",
    "    req = session.get(url)\n",
    "    req.raise_for_status()    \n",
    "\n",
    "    body=req.text\n",
    "\n",
    "    soup=BeautifulSoup(body,'html.parser')\n",
    "    awards=soup.find_all('tr')\n",
    "    if awards is not None:\n",
    "        year_buf = []\n",
    "        w_n_buf = []\n",
    "        category_buf = []\n",
    "        for award in awards:\n",
    "            award_record = {\n",
    "                'nconst': nconst,\n",
    "                'year': None,\n",
    "                'category': None,\n",
    "                'w_n': None,\n",
    "                'description': None,\n",
    "                'movie': None,\n",
    "                'tconst': None\n",
    "            } \n",
    "            if award.find('td', class_='award_year') is not None:\n",
    "                award_year_td = award.find('td', class_='award_year')\n",
    "                if award_year_td.find('a') is not None:\n",
    "\n",
    "                    year = award.find('td', class_='award_year').find('a').text\n",
    "                    if year is not None:\n",
    "                        try:\n",
    "                            year = int(year.replace(\"\\n\", \"\").strip())\n",
    "                            if year:\n",
    "                                award_record['year'] = year\n",
    "\n",
    "                            # If the td has a row span more than 1, cache the value so it will be used \n",
    "                            # with the subsequent corresponding trs as well\n",
    "                            if award_year_td['rowspan'] is not None:\n",
    "                                award_year_td_rs = int(award_year_td['rowspan'])\n",
    "                                for i in range(0, award_year_td_rs - 1):\n",
    "                                    year_buf.append(year)\n",
    "                        except:\n",
    "                            print(\"Failed to parse int year {0}\".format(year))\n",
    "\n",
    "            elif len(year_buf) > 0:\n",
    "                buffed_year = year_buf.pop()\n",
    "                if buffed_year:\n",
    "                    award_record['year'] = buffed_year\n",
    "\n",
    "            if award.find('td',class_='award_outcome') is not None:\n",
    "                award_outcome_td = award.find('td',class_='award_outcome')\n",
    "                if award_outcome_td.find('span',class_='award_category') is not None:\n",
    "                    award_cat = award_outcome_td.find('span',class_='award_category').text\n",
    "                    if award_cat:\n",
    "                        award_record['category'] = award_cat\n",
    "\n",
    "                if award_outcome_td.find('b') is not None:\n",
    "                    w_n_txt = award_outcome_td.find('b').text\n",
    "                    if w_n_txt is not None:\n",
    "                        w_n = w_n_txt.replace(\"\\n\", \"\").strip()\n",
    "                        if w_n:\n",
    "                            award_record['w_n'] = w_n\n",
    "\n",
    "                if award_outcome_td['rowspan'] is not None:\n",
    "                    award_outcome_td_rs = int(award_outcome_td['rowspan'])\n",
    "                    for i in range(0, award_outcome_td_rs - 1):\n",
    "                        category_buf.append(award_cat)\n",
    "                        w_n_buf.append(w_n)\n",
    "            else:\n",
    "                if len(w_n_buf) > 0:\n",
    "                    buffed_w_n = w_n_buf.pop()\n",
    "                    if buffed_w_n:\n",
    "                        award_record['w_n'] = buffed_w_n\n",
    "                if len(category_buf) > 0:\n",
    "                    buffed_category = category_buf.pop()\n",
    "                    if buffed_category:\n",
    "                        award_record['category'] = buffed_category\n",
    "\n",
    "\n",
    "            if award.find('td', class_='award_description') is not None:\n",
    "                award_txt = award.find('td',class_='award_description').find(text=True, recursive=False)\n",
    "                award_info = award.find('td',class_='award_description').find('a', href=re.compile(r'.*tt\\d{7,8}.*'))\n",
    "                if award_txt is not None:\n",
    "                    desc_txt = award_txt.replace(\"\\n\", \"\").strip()\n",
    "                    if desc_txt:\n",
    "                        award_record['description'] = desc_txt\n",
    "\n",
    "                if award_info is not None:\n",
    "                    tconst_res = re.search('tt\\d{7,8}', award_info.get('href'))\n",
    "                    if tconst_res is not None:\n",
    "                        tconst_val = tconst_res.group(0)\n",
    "                        if tconst_val:\n",
    "                            award_record['tconst'] = tconst_val\n",
    "                        award_info_txt = award_info.text\n",
    "                        if award_info_txt:\n",
    "                            award_record['movie'] = award_info.text\n",
    "            result.append(award_record)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the start index and end index. Ip is renewed after each chunk of urls of 'step_size'. \n",
    "# After every chunk, result is written to a pickle. So if you need to stop the execution in the middle, note the\n",
    "# index range of the last successfully written pickle file (from the printed logs) and use the remaining range\n",
    "# for start and end index to resume from where you stopped.\n",
    "postfix = 'stars'\n",
    "s_idx = 0\n",
    "e_idx = len(actors)\n",
    "step_size = 4000\n",
    "\n",
    "print(\"Starting scraping...\")\n",
    "\n",
    "base_url = 'https://www.imdb.com/name/{0}/awards'\n",
    "urls=[]\n",
    "for index, row in actors[s_idx:e_idx].iterrows():\n",
    "    urls.append(base_url.format(row['nconst']))\n",
    "\n",
    "used_ips = []\n",
    "failed_urls = []\n",
    "dropped_urls = []\n",
    "\n",
    "retry_counts = collections.defaultdict(int)\n",
    "\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=100) as executor:\n",
    "    session = get_tor_session()\n",
    "    chunk_indeces = np.arange(0, len(urls), step_size)\n",
    "    \n",
    "    for idx, start in enumerate(chunk_indeces, start=1):\n",
    "        start_t = time.time()\n",
    "        finals = []\n",
    "        renew_connection()\n",
    "        session = get_tor_session()\n",
    "        new_ip = ast.literal_eval(session.get(\"http://httpbin.org/ip\").text)[\"origin\"].split(\",\")[0]\n",
    "        while new_ip in used_ips:\n",
    "            print(\"Renewed IP {0} already used. Waiting 5s to renew...\".format(new_ip))\n",
    "            time.sleep(5)\n",
    "            renew_connection()\n",
    "            session = get_tor_session()\n",
    "            new_ip = ast.literal_eval(session.get(\"http://httpbin.org/ip\").text)[\"origin\"].split(\",\")[0]\n",
    "        used_ips.append(new_ip)\n",
    "        \n",
    "        url_chunk = urls[start:start + step_size] + failed_urls\n",
    "        failed_urls = []\n",
    "        \n",
    "        print(\"Submitting {0} URLs to the executor\".format(len(url_chunk)))\n",
    "        \n",
    "        future_to_url = {executor.submit(get_data, session, url): url for url in url_chunk}\n",
    "        for future in concurrent.futures.as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                data = future.result()\n",
    "                finals += data\n",
    "            except Exception as exc:\n",
    "                stripped = url.strip(\"'\")\n",
    "                # Retry only once\n",
    "                if retry_counts[stripped] == 0:\n",
    "                    failed_urls.append(stripped)\n",
    "                    retry_counts[stripped] = 1\n",
    "                    print('%s generated an exception: %s. Added to retry list.' % (url, exc))\n",
    "#                     exc_info = sys.exc_info()\n",
    "#                     traceback.print_exception(*exc_info)\n",
    "                else:\n",
    "                    print('%s generated an exception: %s. Dropping since retry failed.' % (url, exc))\n",
    "                    dropped_urls.append(stripped)\n",
    "#                     exc_info = sys.exc_info()\n",
    "#                     traceback.print_exception(*exc_info)\n",
    "\n",
    "        \n",
    "        concurrent.futures.wait(\n",
    "            list(future_to_url.keys()), \n",
    "            timeout=None, \n",
    "            return_when=concurrent.futures.ALL_COMPLETED)\n",
    "        end_t = time.time()\n",
    "        print(\"chunk {0} of {1} completed with IP {2}. Took {3} seconds.\"\\\n",
    "              .format(idx, len(chunk_indeces), new_ip, end_t - start_t))   \n",
    "        actors_df = pd.DataFrame(data=finals)\n",
    "        acror_pickle_save_dir = './pickles/actor'\n",
    "        if not os.path.exists(acror_pickle_save_dir):\n",
    "            os.makedirs(acror_pickle_save_dir)\n",
    "        pickle_path = \"{4}/{0}_{1}-{2}_{3}.pkl\"\\\n",
    "                        .format(idx, start, start + step_size, postfix, acror_pickle_save_dir)\n",
    "        actors_df.to_pickle(pickle_path)\n",
    "        print(\"Wrote pickle {0} with {1} rows\".format(pickle_path, len(actors_df)))\n",
    "        \n",
    "    if len(failed_urls) > 0:\n",
    "        print('\\n\\n\\n\\n!!!**********************************!!!')\n",
    "        print('Failed to retrieve the following URLs.\\n')\n",
    "        print(failed_urls)\n",
    "        print('\\n!!!**********************************!!!')\n",
    "        print('Note: Run the following cell to retry the failed urls')\n",
    "    else:\n",
    "        print(\"\\n\\n\\n\\n!!!**********************************!!!\\n\")\n",
    "        print(\"Scraping succeeded!\")\n",
    "        print('\\n!!!**********************************!!!')\n",
    "        print('''Note: Next, run the script that combine all pickles''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Run the following script only if there were any failed URLS at the end of the previous script execution.\n",
    "Run this several times if you still see failures until there are none. But if some dont succeed even \n",
    "after multiple attempts, its probably a problem on imdb side and we can just ignore them.\n",
    "'''\n",
    "urls = failed_urls\n",
    "if len(urls) > 0:\n",
    "    retry_failed_urls = []\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=150) as executor:\n",
    "    session = get_tor_session(None)\n",
    "    chunk_indeces = np.arange(0, len(urls), step_size)\n",
    "\n",
    "    for idx, start in enumerate(chunk_indeces, start=1):\n",
    "        start_t = time.time()\n",
    "        finals = []\n",
    "        renew_connection()\n",
    "        session = get_tor_session(session)\n",
    "        new_ip = ast.literal_eval(session.get(\"http://httpbin.org/ip\").text)[\"origin\"].split(\",\")[0]\n",
    "        while new_ip in used_ips:\n",
    "            print(\"Renewed IP {0} already used. Waiting 5s to renew...\".format(new_ip))\n",
    "            time.sleep(5)\n",
    "            renew_connection()\n",
    "            session = get_tor_session(session)\n",
    "            new_ip = ast.literal_eval(session.get(\"http://httpbin.org/ip\").text)[\"origin\"].split(\",\")[0]\n",
    "        used_ips.append(new_ip)\n",
    "\n",
    "        url_chunk = urls[start:start + step_size] + retry_failed_urls\n",
    "        retry_failed_urls = []\n",
    "\n",
    "        future_to_url = {executor.submit(get_data, session, url): url for url in url_chunk}\n",
    "        for future in concurrent.futures.as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                data = future.result()\n",
    "                finals += data\n",
    "            except Exception as exc:\n",
    "                retry_failed_urls.append(url.strip(\"'\"))\n",
    "                print('%s generated an exception: %s. Added to retry list.' % (url, exc))\n",
    "\n",
    "        concurrent.futures.wait(list(future_to_url.keys()), timeout=None, return_when=concurrent.futures.ALL_COMPLETED)\n",
    "        end_t = time.time()\n",
    "        print(\"chunk {0} of {1} completed with IP {2}. Took {3} seconds.\".format(idx, len(chunk_indeces), new_ip, end_t - start_t))   \n",
    "        actors_df = pd.DataFrame(data=finals)\n",
    "        if len(actors_df) > 0:\n",
    "            pickle_path = \"./pickles/actor/{0}_{1}-{2}_{3}_retry.pkl\".format(idx, start, start + step_size, postfix)\n",
    "            actors_df.to_pickle(pickle_path)\n",
    "            print(\"Wrote pickle {0} with {1} rows\".format(pickle_path, len(actors_df)))\n",
    "\n",
    "    if len(retry_failed_urls) > 0:\n",
    "        print('\\n\\n\\n\\n!!!**********************************!!!')\n",
    "        print('Retry failed with the following URLs.\\n')\n",
    "        print(retry_failed_urls)\n",
    "        print('\\n!!!**********************************!!!')\n",
    "        print('Note: Run this script several times to see if all succeed.',\n",
    "        'If some dont, it probably means they wont ever. so just give them up.')\n",
    "    else:\n",
    "        print(\"\\n\\n\\n\\n!!!**********************************!!!\\n\")\n",
    "        print(\"Retry succeeded!\")\n",
    "        print('\\n!!!**********************************!!!')\n",
    "        print('''Note: Run the following script to combine all pickles''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the created pickles into one Dataframe\n",
    "\n",
    "dir_path = \"./pickles/actor/\"\n",
    "# Ignore any sytem files starting with . or folders if any\n",
    "pickles = [f for f in listdir(dir_path) if isfile(join(dir_path, f)) and not f.startswith('.')]\n",
    "\n",
    "dfs = list(map(lambda x: pd.read_pickle(\"{0}/{1}\".format(dir_path,x)), pickles))\n",
    "combined_df = pd.concat(dfs)\n",
    "combined_df.reset_index(drop=True, inplace=True)\n",
    "combined_df.to_pickle(\"./pickles/complete_actors.pkl\".format(e_idx))\n",
    "len(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fix use of arrays in the dictionaries to just values\n",
    "import re\n",
    "import ast\n",
    "import sys\n",
    "import time\n",
    "import requests\n",
    "import traceback\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "import urllib.request\n",
    "import concurrent.futures\n",
    "from os import listdir\n",
    "from stem import Signal\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from os.path import isfile, join\n",
    "from stem.control import Controller\n",
    "\n",
    "pickles_save_dir = \"./pickles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tor_session():\n",
    "    session = requests.session()\n",
    "    # Tor uses the 9050 port as the default socks port\n",
    "    session.proxies = {'http':  'socks5://127.0.0.1:9050',\n",
    "                       'https': 'socks5://127.0.0.1:9050'}\n",
    "    return session\n",
    "\n",
    "def renew_connection():\n",
    "    with Controller.from_port(port = 9051) as controller:\n",
    "        controller.authenticate(password=\"password\")\n",
    "        controller.signal(Signal.NEWNYM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
