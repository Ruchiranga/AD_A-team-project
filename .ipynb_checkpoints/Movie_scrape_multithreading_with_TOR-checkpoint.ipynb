{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fix use of arrays in the dictionaries to just values\n",
    "import re\n",
    "import ast\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "import urllib.request\n",
    "import concurrent.futures\n",
    "from os import listdir\n",
    "from stem import Signal\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from os.path import isfile, join\n",
    "from stem.control import Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tor_session():\n",
    "    session = requests.session()\n",
    "    # Tor uses the 9050 port as the default socks port\n",
    "    session.proxies = {'http':  'socks5://127.0.0.1:9050',\n",
    "                       'https': 'socks5://127.0.0.1:9050'}\n",
    "    return session\n",
    "\n",
    "def renew_connection():\n",
    "    with Controller.from_port(port = 9051) as controller:\n",
    "        controller.authenticate(password=\"password\")\n",
    "        controller.signal(Signal.NEWNYM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tconst</th>\n",
       "      <th>titleType</th>\n",
       "      <th>primaryTitle</th>\n",
       "      <th>originalTitle</th>\n",
       "      <th>isAdult</th>\n",
       "      <th>startYear</th>\n",
       "      <th>endYear</th>\n",
       "      <th>runtimeMinutes</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tt0000009</td>\n",
       "      <td>movie</td>\n",
       "      <td>Miss Jerry</td>\n",
       "      <td>Miss Jerry</td>\n",
       "      <td>0</td>\n",
       "      <td>1894</td>\n",
       "      <td>\\N</td>\n",
       "      <td>45</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>tt0000147</td>\n",
       "      <td>movie</td>\n",
       "      <td>The Corbett-Fitzsimmons Fight</td>\n",
       "      <td>The Corbett-Fitzsimmons Fight</td>\n",
       "      <td>0</td>\n",
       "      <td>1897</td>\n",
       "      <td>\\N</td>\n",
       "      <td>20</td>\n",
       "      <td>Documentary,News,Sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>tt0000335</td>\n",
       "      <td>movie</td>\n",
       "      <td>Soldiers of the Cross</td>\n",
       "      <td>Soldiers of the Cross</td>\n",
       "      <td>0</td>\n",
       "      <td>1900</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>Biography,Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>tt0000502</td>\n",
       "      <td>movie</td>\n",
       "      <td>Bohemios</td>\n",
       "      <td>Bohemios</td>\n",
       "      <td>0</td>\n",
       "      <td>1905</td>\n",
       "      <td>\\N</td>\n",
       "      <td>100</td>\n",
       "      <td>\\N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>tt0000574</td>\n",
       "      <td>movie</td>\n",
       "      <td>The Story of the Kelly Gang</td>\n",
       "      <td>The Story of the Kelly Gang</td>\n",
       "      <td>0</td>\n",
       "      <td>1906</td>\n",
       "      <td>\\N</td>\n",
       "      <td>70</td>\n",
       "      <td>Biography,Crime,Drama</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        tconst titleType                   primaryTitle  \\\n",
       "8    tt0000009     movie                     Miss Jerry   \n",
       "145  tt0000147     movie  The Corbett-Fitzsimmons Fight   \n",
       "332  tt0000335     movie          Soldiers of the Cross   \n",
       "499  tt0000502     movie                       Bohemios   \n",
       "571  tt0000574     movie    The Story of the Kelly Gang   \n",
       "\n",
       "                     originalTitle  isAdult startYear endYear runtimeMinutes  \\\n",
       "8                       Miss Jerry        0      1894      \\N             45   \n",
       "145  The Corbett-Fitzsimmons Fight        0      1897      \\N             20   \n",
       "332          Soldiers of the Cross        0      1900      \\N             \\N   \n",
       "499                       Bohemios        0      1905      \\N            100   \n",
       "571    The Story of the Kelly Gang        0      1906      \\N             70   \n",
       "\n",
       "                     genres  \n",
       "8                   Romance  \n",
       "145  Documentary,News,Sport  \n",
       "332         Biography,Drama  \n",
       "499                      \\N  \n",
       "571   Biography,Crime,Drama  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.exists('./filtered_movies.pkl'):\n",
    "    movies = pd.read_pickle(\"./filtered_movies.pkl\")\n",
    "else:\n",
    "    title_basics = pd.read_csv(\"data/title.basics.tsv\", sep='\\t')\n",
    "    movies = title_basics[title_basics.titleType == 'movie']\n",
    "    movies.to_pickle(\"./filtered_movies.pkl\")\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "534354"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(session, url):\n",
    "    scraped_data = {\n",
    "    \"tconst\": [],\n",
    "    \"stars\": [],\n",
    "    \"oscarWins\": [],\n",
    "    \"nominations\": [],\n",
    "    \"wins\": [],\n",
    "    \"releaseDate\": [],\n",
    "    \"releaseCountry\": [],\n",
    "    \"plotKeywords\": [],\n",
    "    \"budget\": [],\n",
    "    \"worldwideGross\": [],\n",
    "    \"metascore\": [],\n",
    "    \"musicProducer\": []\n",
    "    }\n",
    "    \n",
    "    info=[]\n",
    "    \n",
    "    try:\n",
    "        r = session.get(url)\n",
    "        r.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as err:\n",
    "        print(err)\n",
    "    page_body = r.text\n",
    "    soup = BeautifulSoup(page_body, 'html.parser')\n",
    "    \n",
    "    #tconst\n",
    "    tconst=url.rsplit('/', 1)[-1]\n",
    "    scraped_data['tconst'].append(tconst)\n",
    "    \n",
    "    \n",
    "    with open(\"./ScrapedPages/tor/{0}.html\".format(tconst), \"w\") as text_file:\n",
    "        text_file.write(page_body)\n",
    "        \n",
    "    # Stars\n",
    "    stars = []\n",
    "    stars_h4 = soup.find('h4', string='Stars:')\n",
    "    if stars_h4 is not None:\n",
    "        star_atags_parent = stars_h4.parent\n",
    "        if star_atags_parent is not None:\n",
    "            star_atags = star_atags_parent.find_all('a')\n",
    "            if star_atags is not None:\n",
    "                for atag in star_atags:\n",
    "                    if atag['href'].startswith('/name/'):\n",
    "                        stars.append(atag['href'].split('/')[2])\n",
    "    else:\n",
    "        stars_h4 = soup.find('h4', string='Star:')\n",
    "        if stars_h4 is not None:\n",
    "            star_atags_parent = stars_h4.parent\n",
    "            if star_atags_parent is not None:\n",
    "                star_atags = star_atags_parent.find_all('a')\n",
    "                if star_atags is not None:\n",
    "                    for atag in star_atags:\n",
    "                        if atag['href'].startswith('/name/'):\n",
    "                            stars.append(atag['href'].split('/')[2])\n",
    "    scraped_data['stars'].append(stars)\n",
    "    \n",
    "    \n",
    "    # Metascore\n",
    "    metascore = None\n",
    "    metascore_list = soup.select('.metacriticScore span:first-child')\n",
    "    if len(metascore_list) > 0:\n",
    "        metascore = metascore_list[0].string\n",
    "        \n",
    "    scraped_data['metascore'].append(metascore if metascore is None else str(metascore))\n",
    "    \n",
    "    \n",
    "    #awards\n",
    "    awrds_lines = soup.find_all(class_=\"awards-blurb\")\n",
    "    oscars = 0\n",
    "    wins = 0\n",
    "    nominations = 0\n",
    "    for line in awrds_lines:\n",
    "        \n",
    "        if line.findChild() is not None:\n",
    "            prepped_str = re.sub(' +', ' ', line.findChild().text.replace(\"\\n\", \" \").strip())\n",
    "            res = re.search('(W|w)on (\\d+) (O|o)scars.?', prepped_str)\n",
    "            if res is not None:\n",
    "                oscars = int(res.group(2))\n",
    "            \n",
    "        else:\n",
    "            prepped_str = re.sub(' +', ' ', line.text.replace(\"\\n\", \"\").strip())\n",
    "            \n",
    "            res = re.search('(\\d+) wins', prepped_str)\n",
    "            if res is not None:\n",
    "                wins = int(res.group(1))\n",
    "            \n",
    "            \n",
    "            res = re.search('(\\d+) nominations', prepped_str)\n",
    "            if res is not None:\n",
    "                nominations = int(res.group(1))\n",
    "    scraped_data['oscarWins'].append(oscars)\n",
    "    scraped_data['wins'].append(wins)\n",
    "    scraped_data['nominations'].append(nominations)\n",
    "    \n",
    "    \n",
    "    # Release date\n",
    "    release_date_h4 = soup.find('h4', string='Release Date:')\n",
    "    release_date = None\n",
    "    release_country = None\n",
    "    if release_date_h4 is not None:\n",
    "        release_date_raw_text = release_date_h4.parent.findAll(text=True, recursive=False)\n",
    "        release_date_prepped = re.sub(' +', ' ', ''.join(release_date_raw_text).replace(\"\\n\", \"\").strip())\n",
    "        date_str_match = re.search(r'\\d{1,2} \\w+ \\d{4}', release_date_prepped)\n",
    "        if date_str_match is not None:\n",
    "            release_date = datetime.strptime(date_str_match.group(), '%d %B %Y').date()\n",
    "        release_country_match = re.search(r'\\(([a-zA-Z ]{2,})\\)', release_date_prepped)\n",
    "        if release_country_match is not None and len(release_country_match.groups()) > 0:\n",
    "            release_country = release_country_match.group(1)\n",
    "        \n",
    "    scraped_data['releaseDate'].append(release_date)\n",
    "    scraped_data['releaseCountry'].append(release_country)\n",
    "    \n",
    "    \n",
    "    # Budget\n",
    "    budget_h4 = soup.find('h4', string='Budget:')\n",
    "    budget = None\n",
    "    if budget_h4 is not None:\n",
    "        budget_raw_text = budget_h4.parent.findAll(text=True, recursive=False)\n",
    "        budget = re.sub(' +', ' ', ''.join(budget_raw_text).replace(\"\\n\", \"\").strip())\n",
    "        \n",
    "    scraped_data['budget'].append(budget)\n",
    "    \n",
    "    \n",
    "    # worldwide gross\n",
    "    gross_h4 = soup.find('h4', string='Cumulative Worldwide Gross:')\n",
    "    gross = None\n",
    "    if gross_h4 is not None:\n",
    "        gross_h4_text = gross_h4.parent.findAll(text=True, recursive=False)\n",
    "        gross = re.sub(' +', ' ', ''.join(gross_h4_text).replace(\"\\n\", \"\").strip())\n",
    "    \n",
    "    scraped_data['worldwideGross'].append(gross)\n",
    "    \n",
    "    \n",
    "    # Plot keywords\n",
    "    keywords_verification_threshold = 2 # Consider only words atleast 2 people considered relavent\n",
    "    keywords_url = url + \"/keywords\"\n",
    "    r = get(keywords_url)\n",
    "    page_body = r.text\n",
    "    soup = BeautifulSoup(page_body, 'html.parser')\n",
    "    keywords = []\n",
    "    plot_keywords_items = soup.find_all(class_=\"soda sodavote\")\n",
    "    if plot_keywords_items is not None:\n",
    "        for plot_keywords_item in plot_keywords_items:\n",
    "            validity_text = plot_keywords_item.find(class_='interesting-count-text').a.text.strip()\n",
    "            validity_text_match = re.search(r'(\\d+) of', validity_text)\n",
    "            if validity_text_match is not None and len(validity_text_match.groups()) > 0:\n",
    "                if int(validity_text_match.group(1)) >= keywords_verification_threshold:\n",
    "                    keywords.append(plot_keywords_item.find(class_='sodatext').a.text.strip())\n",
    "    \n",
    "    scraped_data['plotKeywords'].append(keywords)\n",
    "    \n",
    "    \n",
    "    # Music producer\n",
    "    fullcredits_url = url + \"/fullcredits\"\n",
    "    r = get(fullcredits_url)\n",
    "    page_body = r.text\n",
    "    soup = BeautifulSoup(page_body, 'html.parser')\n",
    "    \n",
    "    music_producer = None\n",
    "    \n",
    "    full_credits_container = soup.find(id='fullcredits_content', class_='header')\n",
    "    if full_credits_container is not None:\n",
    "        full_credits = full_credits_container.find_all(recursive=False)\n",
    "        if full_credits is not None:\n",
    "            for idx, item in enumerate(full_credits, start=0):\n",
    "                if 'Music by' in item.text:\n",
    "                    producer_atag = full_credits[idx + 1].find('a')\n",
    "                    if producer_atag is not None:\n",
    "                        producer_href = producer_atag['href']\n",
    "                        if producer_href is not None:\n",
    "                            music_producer = producer_href.split('/')[2]\n",
    "                            break\n",
    "    \n",
    "    scraped_data['musicProducer'].append(music_producer)\n",
    "    return scraped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set the start index and end index. Ip is renewed after each chunk of urls of 'step_size'. \n",
    "# After every chunk, result is written to a pickle. So if you need to stop the execution in the middle, note the\n",
    "# index range of the last successfully written pickle file (from the printed logs) and use the remaining range\n",
    "# for start and end index to resume from where you stopped.\n",
    "postfix = 'tor'\n",
    "s_idx = 0\n",
    "e_idx = 133588\n",
    "step_size = 4000\n",
    "# [0:133588]\n",
    "\n",
    "base_url = 'https://www.imdb.com/title/'\n",
    "urls=[]\n",
    "for index, row in movies[s_idx:e_idx].iterrows():\n",
    "    urls.append(base_url + row['tconst'])\n",
    "\n",
    "used_ips = []\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=150) as executor:\n",
    "    session = get_tor_session()\n",
    "    chunk_indeces = np.arange(0, len(urls), step_size)\n",
    "    failed_urls = []\n",
    "    \n",
    "    for idx, start in enumerate(chunk_indeces, start=1):\n",
    "        start_t = time.time()\n",
    "        finals = {\n",
    "            \"tconst\": [],\n",
    "            \"stars\": [],\n",
    "            \"oscarWins\": [],\n",
    "            \"nominations\": [],\n",
    "            \"wins\": [],\n",
    "            \"releaseDate\": [],\n",
    "            \"releaseCountry\": [],\n",
    "            \"plotKeywords\": [],\n",
    "            \"budget\": [],\n",
    "            \"worldwideGross\": [],\n",
    "            \"metascore\": [],\n",
    "            \"musicProducer\": []\n",
    "        }\n",
    "        renew_connection()\n",
    "        session = get_tor_session()\n",
    "        new_ip = ast.literal_eval(session.get(\"http://httpbin.org/ip\").text)[\"origin\"].split(\",\")[0]\n",
    "        while new_ip in used_ips:\n",
    "            print(\"Renewed IP {0} already used. Waiting 5s to renew...\".format(new_ip))\n",
    "            time.sleep(5)\n",
    "            renew_connection()\n",
    "            session = get_tor_session()\n",
    "            new_ip = ast.literal_eval(session.get(\"http://httpbin.org/ip\").text)[\"origin\"].split(\",\")[0]\n",
    "        used_ips.append(new_ip)\n",
    "        \n",
    "        url_chunk = urls[start:start + step_size] + failed_urls\n",
    "        failed_urls = []\n",
    "        \n",
    "        future_to_url = {executor.submit(get_data, session, url): url for url in url_chunk}\n",
    "        for future in concurrent.futures.as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                data = future.result()\n",
    "                for k, v in data.items():\n",
    "                    finals[k].append(v[0])\n",
    "            except Exception as exc:\n",
    "                failed_urls.append(url.strip(\"'\"))\n",
    "                print('%s generated an exception: %s. Added to retry list.' % (url, exc))\n",
    "        \n",
    "        concurrent.futures.wait(list(future_to_url.keys()), timeout=None, return_when=concurrent.futures.ALL_COMPLETED)\n",
    "        end_t = time.time()\n",
    "        print(\"chunk {0} of {1} completed with IP {2}. Took {3} seconds.\".format(idx, len(chunk_indeces), new_ip, end_t - start_t))   \n",
    "        movies_df = pd.DataFrame(data=finals)\n",
    "        pickle_path = \"./pickles/tor/{0}_{1}-{2}_{3}.pkl\".format(idx, start, start + step_size, postfix)\n",
    "        movies_df.to_pickle(pickle_path)\n",
    "        print(\"Wrote pickle {0} with {1} rows\".format(pickle_path, len(movies_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the created pickles into one Dataframe\n",
    "\n",
    "dir_path = \"./pickles/tor\"\n",
    "# Ignore any sytem files starting with . or folders if any\n",
    "pickles = [f for f in listdir(dir_path) if isfile(join(dir_path, f)) and not f.startswith('.')]\n",
    "dfs = list(map(lambda x: pd.read_pickle(\"{0}/{1}\".format(dir_path,x)), pickles))\n",
    "combined_df = pd.concat(dfs)\n",
    "combined_df.reset_index(drop=True, inplace=True)\n",
    "combined_df.to_pickle(\"./pickles/complete_{0}.pkl\".format(e_idx))\n",
    "with pd.option_context('display.max_rows', 50000, 'display.max_columns', 20):\n",
    "    display(combined_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_ctr = {\n",
    "#     \"tconst\": [],\n",
    "#     \"stars\": [],\n",
    "#     \"oscarWins\": [],\n",
    "#     \"nominations\": [],\n",
    "#     \"wins\": [],\n",
    "#     \"releaseDate\": [],\n",
    "#     \"releaseCountry\": [],\n",
    "#     \"plotKeywords\": [],\n",
    "#     \"budget\": [],\n",
    "#     \"worldwideGross\": [],\n",
    "#     \"metascore\": [],\n",
    "#     \"musicProducer\": []\n",
    "# }\n",
    "\n",
    "# def tempfn(x):\n",
    "#     for index, value in x.items():\n",
    "#         test_ctr[index].append(value[0]) \n",
    "\n",
    "# test_df = part1.apply(tempfn, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renew_connection()\n",
    "# session = get_tor_session()\n",
    "# print(session.get(\"http://httpbin.org/ip\").text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
