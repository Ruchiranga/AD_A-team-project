{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e82bb3cdb57b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmultiprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import os.path\n",
    "import re\n",
    "from datetime import datetime\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "root_path = 'gdrive/My Drive/Ada_data/'\n",
    "\n",
    "if os.path.exists(root_path + 'filtered_movies.pkl'):\n",
    "    movies = pd.read_pickle(root_path + \"filtered_movies.pkl\")\n",
    "else:\n",
    "    title_basics = pd.read_csv(root_path + \"title.basics.tsv\", sep='\\t')\n",
    "    movies = title_basics[title_basics.titleType == 'movie']\n",
    "    movies.to_pickle(root_path + \"filtered_movies.pkl\")\n",
    "\n",
    "base_url = 'https://www.imdb.com/title/'\n",
    "urls=[]\n",
    "\n",
    "part=2\n",
    "s_idx = 26717\n",
    "e_idx = 53435\n",
    "\n",
    "for index, row in movies[s_idx:e_idx].iterrows():\n",
    "    urls.append(base_url + row['tconst'])\n",
    "\n",
    "\n",
    "def get_data(url):\n",
    "    scraped_data = {\n",
    "    \"tconst\": [],\n",
    "    \"stars\": [],\n",
    "    \"oscarWins\": [],\n",
    "    \"nominations\": [],\n",
    "    \"wins\": [],\n",
    "    \"releaseDate\": [],\n",
    "    \"releaseCountry\": [],\n",
    "    \"plotKeywords\": [],\n",
    "    \"budget\": [],\n",
    "    \"worldwideGross\": [],\n",
    "    \"metascore\": [],\n",
    "    \"musicProducer\": []\n",
    "    }\n",
    "    \n",
    "    info=[]\n",
    "    r = get(url)\n",
    "    page_body = r.text\n",
    "    soup = BeautifulSoup(page_body, 'html.parser')\n",
    "    \n",
    "    #tconst\n",
    "    \n",
    "    tconst=url.rsplit('/', 1)[-1]\n",
    "    print(\"Id {0}\".format(tconst))\n",
    "    scraped_data['tconst'].append(tconst)\n",
    "    # Stars\n",
    "    stars = []\n",
    "    stars_h4 = soup.find('h4', string='Stars:')\n",
    "    if stars_h4 is not None:\n",
    "        star_atags_parent = stars_h4.parent\n",
    "        if star_atags_parent is not None:\n",
    "            star_atags = star_atags_parent.find_all('a')\n",
    "            if star_atags is not None:\n",
    "                for atag in star_atags:\n",
    "                    if atag['href'].startswith('/name/'):\n",
    "                        stars.append(atag['href'].split('/')[2])\n",
    "    scraped_data['stars'].append(stars)\n",
    "    \n",
    "    \n",
    "    # Metascore\n",
    "    metascore = None\n",
    "    metascore_list = soup.select('.metacriticScore span:nth-of-type(1)')\n",
    "    if len(metascore_list) > 0:\n",
    "        metascore = metascore_list[0].string\n",
    "        \n",
    "    scraped_data['metascore'].append(metascore)\n",
    "    \n",
    "    \n",
    "    #awards\n",
    "    awrds_lines = soup.find_all(class_=\"awards-blurb\")\n",
    "    oscars = 0\n",
    "    wins = 0\n",
    "    nominations = 0\n",
    "    for line in awrds_lines:\n",
    "        \n",
    "        if line.findChild() is not None:\n",
    "            prepped_str = re.sub(' +', ' ', line.findChild().text.replace(\"\\n\", \" \").strip())\n",
    "            res = re.search('(W|w)on (\\d+) (O|o)scars.?', prepped_str)\n",
    "            if res is not None:\n",
    "                oscars = int(res.group(2))\n",
    "            \n",
    "        else:\n",
    "            prepped_str = re.sub(' +', ' ', line.text.replace(\"\\n\", \"\").strip())\n",
    "            \n",
    "            res = re.search('(\\d+) wins', prepped_str)\n",
    "            if res is not None:\n",
    "                wins = int(res.group(1))\n",
    "            \n",
    "            \n",
    "            res = re.search('(\\d+) nominations', prepped_str)\n",
    "            if res is not None:\n",
    "                nominations = int(res.group(1))\n",
    "    scraped_data['oscarWins'].append(oscars)\n",
    "    scraped_data['wins'].append(wins)\n",
    "    scraped_data['nominations'].append(nominations)\n",
    "    \n",
    "    \n",
    "    # Release date\n",
    "    release_date_h4 = soup.find('h4', string='Release Date:')\n",
    "    release_date = None\n",
    "    release_country = None\n",
    "    if release_date_h4 is not None:\n",
    "        release_date_raw_text = release_date_h4.parent.findAll(text=True, recursive=False)\n",
    "        release_date_prepped = re.sub(' +', ' ', ''.join(release_date_raw_text).replace(\"\\n\", \"\").strip())\n",
    "        date_str_match = re.search(r'\\d{1,2} \\w+ \\d{4}', release_date_prepped)\n",
    "        if date_str_match is not None:\n",
    "            release_date = datetime.strptime(date_str_match.group(), '%d %B %Y').date()\n",
    "        release_country_match = re.search(r'\\(([a-zA-Z ]{2,})\\)', release_date_prepped)\n",
    "        if release_country_match is not None and len(release_country_match.groups()) > 0:\n",
    "            release_country = release_country_match.group(1)\n",
    "        \n",
    "    scraped_data['releaseDate'].append(release_date)\n",
    "    scraped_data['releaseCountry'].append(release_country)\n",
    "    \n",
    "    \n",
    "    # Budget\n",
    "    budget_h4 = soup.find('h4', string='Budget:')\n",
    "    budget = None\n",
    "    if budget_h4 is not None:\n",
    "        budget_raw_text = budget_h4.parent.findAll(text=True, recursive=False)\n",
    "        budget = re.sub(' +', ' ', ''.join(budget_raw_text).replace(\"\\n\", \"\").strip())\n",
    "        \n",
    "    scraped_data['budget'].append(budget)\n",
    "    \n",
    "    \n",
    "    # worldwide gross\n",
    "    gross_h4 = soup.find('h4', string='Cumulative Worldwide Gross:')\n",
    "    gross = None\n",
    "    if gross_h4 is not None:\n",
    "        gross_h4_text = gross_h4.parent.findAll(text=True, recursive=False)\n",
    "        gross = re.sub(' +', ' ', ''.join(gross_h4_text).replace(\"\\n\", \"\").strip())\n",
    "    \n",
    "    scraped_data['worldwideGross'].append(gross)\n",
    "    \n",
    "    \n",
    "    # Plot keywords\n",
    "    keywords_verification_threshold = 2 # Consider only words atleast 2 people considered relavent\n",
    "    keywords_url = url + \"/keywords\"\n",
    "    r = get(keywords_url)\n",
    "    page_body = r.text\n",
    "    soup = BeautifulSoup(page_body, 'html.parser')\n",
    "    keywords = []\n",
    "    plot_keywords_items = soup.find_all(class_=\"soda sodavote\")\n",
    "    if plot_keywords_items is not None:\n",
    "        for plot_keywords_item in plot_keywords_items:\n",
    "            validity_text = plot_keywords_item.find(class_='interesting-count-text').a.text.strip()\n",
    "            validity_text_match = re.search(r'(\\d+) of', validity_text)\n",
    "            if validity_text_match is not None and len(validity_text_match.groups()) > 0:\n",
    "                if int(validity_text_match.group(1)) >= keywords_verification_threshold:\n",
    "                    keywords.append(plot_keywords_item.find(class_='sodatext').a.text.strip())\n",
    "    \n",
    "    scraped_data['plotKeywords'].append(keywords)\n",
    "    \n",
    "    \n",
    "    # Music producer\n",
    "    fullcredits_url = url + \"/fullcredits\"\n",
    "    r = get(fullcredits_url)\n",
    "    page_body = r.text\n",
    "    soup = BeautifulSoup(page_body, 'html.parser')\n",
    "    \n",
    "    music_producer = None\n",
    "    \n",
    "    full_credits_container = soup.find(id='fullcredits_content', class_='header')\n",
    "    if full_credits_container is not None:\n",
    "        full_credits = full_credits_container.find_all(recursive=False)\n",
    "        if full_credits is not None:\n",
    "            for idx, item in enumerate(full_credits, start=0):\n",
    "                if 'Music by' in item.text:\n",
    "                    producer_atag = full_credits[idx + 1].find('a')\n",
    "                    if producer_atag is not None:\n",
    "                        producer_href = producer_atag['href']\n",
    "                        if producer_href is not None:\n",
    "                            music_producer = producer_href.split('/')[2]\n",
    "                            break\n",
    "    \n",
    "    scraped_data['musicProducer'].append(music_producer)\n",
    "    return scraped_data\n",
    "\n",
    "\n",
    "p=Pool(20)\n",
    "start = time.time()\n",
    "with Pool(20) as p:\n",
    "    data=p.map(get_data,urls)\n",
    "    # print(data)\n",
    "    p.terminate()\n",
    "    p.join()\n",
    "    movies_df = pd.DataFrame(data=data)\n",
    "    movies_df.to_pickle(root_path + str(part) + \".pkl\")\n",
    "end = time.time()\n",
    "print('Time taken: %f seconds' % (end - start) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
